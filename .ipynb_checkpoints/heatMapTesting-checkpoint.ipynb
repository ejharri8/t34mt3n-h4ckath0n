{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30000 validated image filenames.\n",
      "Found 20000 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "#import statements\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Activation,MaxPooling2D,Dropout,Flatten\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import optimizers\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import os\n",
    "from keras import backend as K\n",
    "\n",
    "#df is data from csv file\n",
    "filename_train_csv = \"data/train_free_throws.csv\"\n",
    "df=pd.read_csv(filename_train_csv, header = None)\n",
    "\n",
    "#change this csv for testing\n",
    "filename_test_csv = \"data/test_free_throws.csv\"\n",
    "mainDF=pd.read_csv(filename_test_csv, header = None)\n",
    "datagen=ImageDataGenerator() #rescale=1./255)\n",
    "\n",
    "\n",
    "df.columns = ['0','1']\n",
    "mainDF.columns = ['0','1']\n",
    "\n",
    "\n",
    "# main generator is the one for test data\n",
    "main_generator=datagen.flow_from_dataframe(dataframe=mainDF, directory=\"data/\", x_col=\"0\",\n",
    "                                           y_col=\"1\", class_mode=\"raw\", target_size=(320,180), batch_size=32)\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(dataframe=df, directory=\"data/\", x_col=\"0\",\n",
    "                                           y_col=\"1\", class_mode=\"raw\", target_size=(320,180), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kernel_size = (4,4)\n",
    "input_shape=(320,180,3)\n",
    "num_classes = 2  # True of False classification\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(6, kernel_size=model_kernel_size, activation='relu', input_shape= input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(16, kernel_size= model_kernel_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dense(84, activation='relu'))\n",
    "\n",
    "final = model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "#heat map\n",
    "def get_model():\n",
    "    model = VGG16_convolutions()\n",
    "    model = load_model_weights(model, \"model.h5\")\n",
    "\n",
    "    model.add(Lambda(global_average_pooling, \n",
    "              output_shape=global_average_pooling_shape))\n",
    "    model.add(Dense(2, activation = 'softmax', init='uniform'))\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "    model.compile(loss = 'categorical_crossentropy', \\\n",
    "        optimizer = sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def visualize_class_activation_map(model_path, img_path, output_path):\n",
    "    csv_file = 'data/test_free_throws.csv'\n",
    "    #load json and create model\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    #load weights into new model\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    #model = load_model(model_path)\n",
    "    model = loaded_model\n",
    "    original_img = cv2.imread(img_path, 1)\n",
    "    width, height, _ = original_img.shape\n",
    "\n",
    "    #Reshape to the network input shape (3, w, h).\n",
    "    img = np.array([np.transpose(np.float32(original_img), (2, 0, 1))])\n",
    "\n",
    "    #Get the 512 input weights to the softmax.\n",
    "    class_weights = model.layers[-1].get_weights()[0]\n",
    "    final_conv_layer = get_output_layer(model, \"conv5_3\")\n",
    "    get_output = K.function([model.layers[0].input], \\\n",
    "                [final_conv_layer.output, \n",
    "    model.layers[-1].output])\n",
    "    [conv_outputs, predictions] = get_output([img])\n",
    "    conv_outputs = conv_outputs[0, :, :, :]\n",
    "\n",
    "    #Create the class activation map.\n",
    "    cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[1:3])\n",
    "    target_class = 1\n",
    "    for i, w in enumerate(class_weights[:, target_class]):\n",
    "            cam += w * conv_outputs[i, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1027 16:37:34.975816 139620888127296 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1027 16:37:34.981608 139620888127296 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W1027 16:37:35.197863 139620888127296 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1027 16:37:35.281975 139620888127296 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "937/937 [==============================] - 77s 83ms/step - loss: 2.1011 - acc: 0.8696 - precision: 0.1303 - recall: 0.9840 - val_loss: 1.3346 - val_acc: 0.9172 - val_precision: 0.0828 - val_recall: 0.9424\n",
      "test accuracy:  [0.8695971184631803]\n",
      "Epoch 1/1\n",
      "937/937 [==============================] - 74s 79ms/step - loss: 2.0895 - acc: 0.8704 - precision: 0.1296 - recall: 0.9872 - val_loss: 1.3370 - val_acc: 0.9171 - val_precision: 0.0829 - val_recall: 0.9392\n",
      "test accuracy:  [0.870294981313401]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#model.compile(optimizers.rmsprop(lr=0.0001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.compile(optimizers.rmsprop(lr=0.0001),loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\", precision, recall])\n",
    "\n",
    "STEP_SIZE_TRAIN = main_generator.n//main_generator.batch_size\n",
    "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "hist = model.fit_generator(generator=main_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=1)\n",
    "\n",
    "#score = model.evalutate(x_test,y_test, verbose = 0)\n",
    "\n",
    "# training values\n",
    "acc = hist.history['acc']\n",
    "loss_ = hist.history['loss']\n",
    "\n",
    "print(\"test accuracy: \" , acc)\n",
    "model.compile(optimizers.rmsprop(lr=0.0001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.compile(optimizers.rmsprop(lr=0.0001),loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\", precision, recall])\n",
    "\n",
    "STEP_SIZE_TRAIN = main_generator.n//main_generator.batch_size\n",
    "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "hist = model.fit_generator(generator=main_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=1)\n",
    "\n",
    "#score = model.evalutate(x_test,y_test, verbose = 0)\n",
    "\n",
    "# training values\n",
    "acc = hist.history['acc']\n",
    "loss_ = hist.history['loss']\n",
    "\n",
    "print(\"test accuracy: \" , acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000, 2), (20000, 2))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_train_csv = \"data/free_throw_training_data.csv\"\n",
    "df=pd.read_csv(filename_train_csv, header = None)\n",
    "\n",
    "cut = 30000\n",
    "cut_end = cut + 20000\n",
    "df1 = df.iloc[:cut, :]\n",
    "df2 = df.iloc[cut:cut_end, :]\n",
    "df1.shape, df2.shape\n",
    "\n",
    "df1.to_csv(\"data/test_free_throws.csv\", header= None, index=False)\n",
    "df2.to_csv(\"data/train_free_throws.csv\", header= None, index=False)\n",
    "\n",
    "df1.shape, df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "#def inference(csv_file):\n",
    "csv_file = 'data/test_free_throws.csv'\n",
    "#load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "#load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30000 validated image filenames.\n",
      "\n",
      " answer:\n",
      "\n",
      "Test loss: 13.75433248745848\n",
      "Test accuracy: 0.13413820704375667\n",
      "Test recall: 0.13026947705442904\n",
      "Test precision: 0.9839914546068858\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#... read csv file\n",
    "# df is data from csv file\n",
    "filename = csv_file\n",
    "df=pd.read_csv(filename, header = None)\n",
    "df.columns = ['0','1']\n",
    "valid_generator = datagen.flow_from_dataframe(dataframe=df, directory=\"data/\", x_col=\"0\",y_col=\"1\",\n",
    "                                            class_mode=\"raw\", target_size=(320,180), batch_size=32)\n",
    "\n",
    "\n",
    "loaded_model.compile(optimizers.rmsprop(lr=0.0001),loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\", precision, recall])\n",
    "#... use the model to do inference\n",
    "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "score = loaded_model.evaluate_generator(generator = valid_generator, steps = STEP_SIZE_VALID,\n",
    "                                 use_multiprocessing = False, verbose = 0 )\n",
    "\n",
    "print('\\n answer:\\n')\n",
    "print('Test loss:', score[0] )\n",
    "print('Test accuracy:', score[1] )\n",
    "print('Test recall:', score[2] )\n",
    "print('Test precision:', score[3] )\n",
    "print('\\n')\n",
    "\n",
    "#... compute accuracy,recall and precision score using 0.5 threshold\n",
    "#return {'accuracy':accuracy, 'recall':recall, 'precision':precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30000 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "filename_test_csv = \"data/test_free_throws.csv\"\n",
    "mainDF=pd.read_csv(filename_test_csv, header = None)\n",
    "mainDF.columns = ['0','1']\n",
    "datagen=ImageDataGenerator() #rescale=1./255)\n",
    "heatmap_generator=datagen.flow_from_dataframe(dataframe=mainDF, directory=\"data/\", x_col=\"0\",\n",
    "                                           y_col=\"1\", class_mode=\"input\", target_size=(320,180), \n",
    "                                           batch_size=1)\n",
    "\n",
    "\n",
    "x,y = heatmap_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Pillow\n",
    "from keract import get_activations\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "plt.imshow(x[0])\n",
    "plt.imshow(y[0])\n",
    "\n",
    "from PIL import Image\n",
    "import numpy\n",
    "im = Image.open(\"data/10031/jpg/320_180/thumb00087542.jpg\")\n",
    "im.save(\"original.jpg\")\n",
    "np_im = numpy.array(im)\n",
    "print (np_im.shape)\n",
    "np_im = np_im - 18\n",
    "new_im = Image.fromarray(np_im)\n",
    "new_im.save(\"numpy_visual.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def heatmap2d(arr: np.ndarray):\n",
    "    plt.imshow(arr, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "test_array = np.arange(100 * 100).reshape(100, 100)\n",
    "#heatmap2d(test_array)\n",
    "heatmap2d(heatmap_generator.next())\n",
    "\n",
    "#activations = get_activations(loaded_model,x[0])\n",
    "\n",
    "#visualize_class_activation_map(loaded_model,\"data/10031/jpg/320_180/thumb00087542.jpg\", \"ourOutput.jpg\")\n",
    "#image = Image.open(\"data/10031/jpg/320_180/thumb00087542.jpg\")\n",
    "#image.show()\n",
    "#activations = get_activations(loaded_model,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
